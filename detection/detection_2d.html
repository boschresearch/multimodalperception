<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Detection 2D | Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Detection 2D" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Di Feng, Christian Haase-Schuetz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck and Klaus Dietmayer &lt;p&gt; Robert Bosch GmbH in cooperation with Ulm University and Karlruhe Institute of Technology &lt;p&gt; * Contributed equally" />
<meta property="og:description" content="Di Feng, Christian Haase-Schuetz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck and Klaus Dietmayer &lt;p&gt; Robert Bosch GmbH in cooperation with Ulm University and Karlruhe Institute of Technology &lt;p&gt; * Contributed equally" />
<link rel="canonical" href="http://boschresearch.github.io/multimodalperception/detection/detection_2d.html" />
<meta property="og:url" content="http://boschresearch.github.io/multimodalperception/detection/detection_2d.html" />
<meta property="og:site_name" content="Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges" />
<script type="application/ld+json">
{"url":"http://boschresearch.github.io/multimodalperception/detection/detection_2d.html","headline":"Detection 2D","description":"Di Feng, Christian Haase-Schuetz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck and Klaus Dietmayer &lt;p&gt; Robert Bosch GmbH in cooperation with Ulm University and Karlruhe Institute of Technology &lt;p&gt; * Contributed equally","@type":"WebPage","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#FF4747">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="../assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h3 class="project-name">Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges</h3>
      <h4 class="project-tagline">Di Feng*, Christian Haase-Schuetz*, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck and Klaus Dietmayer <p> Robert Bosch GmbH in cooperation with Ulm University and Karlruhe Institute of Technology <p> * Contributed equally</h4>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="detection-2d">Detection 2D</h1>
<p><a id="bck" href="../index.html#introtab"><b>Back to index</b></a>    
<a href="../detection.html#bck"><b>Back to Detection</b></a>    
<a href="../detection/detection_2d.html#bck"><img src="../img/2D.png" alt="2D" width="50" /></a>   
<a href="../detection/detection_3d.html#bck"><img src="../img/3D.png" alt="3D" width="50" /></a>   
<a href="../detection/detection_thermal.html#bck"><img src="../img/Thermal.png" alt="Thermal" width="50" /></a>  
<a href="../detection/detection_lidar.html#bck"><img src="../img/LiDAR.png" alt="LiDAR" width="50" /></a>  
<a href="../detection/detection_radar.html#bck"><img src="../img/Radar.png" alt="Radar" width="50" /></a></p>

<table id="commontab">
<tr><th> Reference </th><th> Sensors </th><th> Object Type </th><th> Sensing Modality Representations and Processing </th><th> Network Pipeline </th><th> How to generate Region Proposals (RP) </th><th> When to fuse </th><th> Fusion Operation and Method </th><th> Fusion Level </th><th> Dataset(s) used </th></tr>

<tr><td valign="top"> Nabati <i>et al.</i>, 2019 
    <a href="https://arxiv.org/pdf/1905.00526.pdf">[pdf]</a><a href="../ref/nabati2019rrpn.bib">[ref]</a>
    </td><td valign="top"> Radar, visual camera </td><td valign="top"> 2D Vehicle </td><td valign="top"> Radar object, RGB image. Radar projected to image frame. </td><td valign="top"> Fast R-CNN </td><td valign="top"> Radar used to generate region proposal </td><td valign="top"> Implicit at RP </td><td valign="top"> Region proposal </td><td valign="top"> Middle </td><td valign="top"> nuScenes </td></tr>

<tr><td valign="top"> Bijelic <i>et al.</i>, 2019 
    <a href="https://arxiv.org/pdf/1902.08913">[pdf]</a><a href="../ref/bijelic2019seeing.bib">[ref]</a>
    </td><td valign="top"> LiDAR, visual camera </td><td valign="top"> 2D Car in foggy weather </td><td valign="top"> Lidar front view images (depth, intensity, height), RGB image. Each processed by VGG16 </td><td valign="top"> SSD </td><td valign="top"> Predictions with fused features </td><td valign="top"> Before RP </td><td valign="top"> Feature concatenation </td><td valign="top"> From early to middle layers </td><td valign="top"> Self-recorded datasets focused on foggy weather, simulated foggy images from KITTI </td></tr>

<tr><td valign="top"> Chadwick <i>et al.</i>, 2019 
    <a href="https://arxiv.org/pdf/1901.10951">[pdf]</a><a href="../ref/chadwick2019distant.bib">[ref]</a>
    </td><td valign="top"> Radar, visual camera </td><td valign="top"> 2D Vehicle </td><td valign="top"> Radar range and velocity maps, RGB image. Each processed by ResNet </td><td valign="top"> One stage detector </td><td valign="top"> Predictions with fused features </td><td valign="top"> Before RP </td><td valign="top"> Addition, feature concatenation </td><td valign="top"> Middle </td><td valign="top"> Self-recorded </td></tr>

<tr><td valign="top"> Pfeuffer <i>et al.</i>, 2018 
    <a href="https://arxiv.org/pdf/1807.02323">[pdf]</a><a href="/ref/pfeuffer2018optimal.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> Multiple 2D objects </td><td valign="top"> LiDAR spherical, and front-view sparse depth, dense depth image, RGB image. Each processed by VGG16 </td><td valign="top"> Faster-RCNN </td><td valign="top"> RPN from fused features </td><td valign="top"> Before RP </td><td valign="top"> Feature concatenation </td><td valign="top"> Early, Middle, Late </td><td valign="top"> KITTI </td></tr>

<tr><td valign="top"> Kim <i>et al.</i>, 2018 
    <a href="https://www.ri.cmu.edu/wp-content/uploads/2017/11/Season-Invariant_Semantic_Segmentation_with_A_Deep_Multimodal_Network.pdf">[pdf]</a><a href="/ref/kim2018season.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Car </td><td valign="top"> LiDAR front-view depth image, RGB image. Each input processed by VGG16 </td><td valign="top"> SSD </td><td valign="top"> SSD with fused features </td><td valign="top"> Before RP </td><td valign="top"> Feature concatenation, Mixture of Experts </td><td valign="top"> Middle </td><td valign="top"> KITTI </td></tr>

<tr><td valign="top"> Guan <i>et al.</i>, 2018 
    <a href="https://arxiv.org/abs/1802.09972">[pdf]</a><a href="/ref/guan2018fusion.bib">[ref]</a>
    </td><td valign="top"> Vision camera, thermal camera </td><td valign="top"> 2D Pedestrian </td><td valign="top"> RGB image, thermal image. Each processed by a base network built on VGG16 </td><td valign="top"> Faster-RCNN </td><td valign="top"> RPN with fused features </td><td valign="top"> Before and after RP </td><td valign="top"> Feature concatenation, Mixture of Experts </td><td valign="top"> Early,  Middle, Late </td><td valign="top"> KAIST Pedestrian Dataset </td></tr>

<tr><td valign="top"> Asvadi <i>et al.</i>, 2017 
    <a href="https://ieeexplore.ieee.org/abstract/document/8317880/">[pdf]</a><a href="/ref/asvadi2017depthcn.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Car </td><td valign="top"> LiDAR front-view dense-depth (DM) and reflectance maps (RM), RGB image. Each processed through a YOLO net </td><td valign="top"> YOLO </td><td valign="top"> YOLO outputs for LiDAR DM and RM maps, and RGB image </td><td valign="top"> After RP </td><td valign="top"> Ensemble: feed engineered features from ensembled bounding boxes to a network to predict scores for NMS </td><td valign="top"> Late </td><td valign="top"> KITTI </td></tr>

<tr><td valign="top"> Oh <i>et al.</i>, 2017 
    <a href="https://www.mdpi.com/1424-8220/17/1/207">[pdf]</a><a href="/ref/oh2017object.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Car, Pedestrian, Cyclist </td><td valign="top"> LiDAR front-view dense-depth map (for fusion: processed by VGG16), LiDAR voxel (for ROIs: segmentation and region growing), RGB image (for fusion: processed by VGG16; for ROIs: segmentation and grouping) </td><td valign="top"> R-CNN </td><td valign="top"> LiDAR voxel and RGB image separately </td><td valign="top"> After RP </td><td valign="top"> Association matrix using basic belief assignment </td><td valign="top"> Late  </td><td valign="top"> KITTI </td></tr>
    
<tr><td valign="top"> Du <i>et al.</i>, 2017 
    <a href="https://ieeexplore.ieee.org/abstract/document/8202234/">[pdf]</a><a href="/ref/du2017car.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Car </td><td valign="top"> LiDAR voxel (processed by RANSAC and model fitting), RGB image (processed by VGG16 and GoogLeNet) </td><td valign="top"> Faster-RCNN </td><td valign="top"> First clustered by LiDAR point clouds, then fine-tuned by a RPN of RGB image </td><td valign="top"> Before RP </td><td valign="top"> Ensemble: feed LiDAR RP to RGB image-based CNN for final prediction  </td><td valign="top"> Late </td><td valign="top"> KITTI </td></tr>
    
<tr><td valign="top"> Schneider <i>et al.</i>, 2017 
    <a href="https://link.springer.com/chapter/10.1007/978-3-319-59126-1_9">[pdf]</a><a href="/ref/schneider2017multimodal.bib">[ref]</a>
    </td><td valign="top"> Vision camera </td><td valign="top"> Multiple 2D objects </td><td valign="top"> RGB image (processed by GoogLeNet), depth image from stereo camera (processed by NiN net) </td><td valign="top"> SSD </td><td valign="top"> SSD predictions. </td><td valign="top"> Before RP </td><td valign="top"> Feature concatenation </td><td valign="top"> Early, Middle, Late </td><td valign="top"> Cityscape </td></tr>

<tr><td valign="top"> Takumi <i>et al.</i>, 2017 
    <a href="https://dl.acm.org/citation.cfm?id=3126727">[pdf]</a><a href="/ref/takumi2017multispectral.bib">[ref]</a>
    </td><td valign="top"> Vision camera, thermal camera </td><td valign="top"> Multiple 2D objects </td><td valign="top"> RGB image, NIR, FIR, FIR image. Each processed by YOLO </td><td valign="top"> YOLO </td><td valign="top"> YOLO predictions for each spectral image </td><td valign="top"> After RP </td><td valign="top"> Ensemble: ensemble final predictions for each YOLO detector </td><td valign="top"> Late </td><td valign="top"> self-recorded data</td></tr>

<tr><td valign="top"> Matti <i>et al.</i>, 2017 
    <a href="https://arxiv.org/abs/1710.06160">[pdf]</a><a href="/ref/matti2017combining.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Pedestrian </td><td valign="top"> LiDAR points (clustering with DBSCAN) and RGB image (processed by ResNet) </td><td valign="top"> R-CNN </td><td valign="top"> Clustered by LiDAR point clouds, then size and ratio corrected on RGB image.  </td><td valign="top"> Before and at RP </td><td valign="top"> Ensemble: feed LiDAR RP to RGB image-based CNN for final prediction </td><td valign="top"> Late </td><td valign="top"> KITTI </td></tr>

<tr><td valign="top"> Schlosser <i>et al.</i>, 2016 
    <a href="https://ieeexplore.ieee.org/abstract/document/7487370">[pdf]</a><a href="/ref/schlosser2016fusing.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Pedestrian  </td><td valign="top"> LiDAR HHA image, RGB image. Each processed by a small ConvNet </td><td valign="top"> R-CNN </td><td valign="top"> Deformable Parts Model with RGB image </td><td valign="top"> After RP </td><td valign="top"> Feature concatenation </td><td valign="top"> Early, Middle, Late </td><td valign="top"> KITTI </td></tr>
    
<tr><td valign="top"> Kim <i>et al.</i>, 2016 
    <a href="https://ieeexplore.ieee.org/abstract/document/7795566/">[pdf]</a><a href="/ref/kim2016robust.bib">[ref]</a>
    </td><td valign="top"> LiDAR, vision camera </td><td valign="top"> 2D Pedestrian, Cyclist </td><td valign="top"> LiDAR front-view depth image, RGB image. Each processed by Fast-RCNN network <a href="/ref/girshick2015fast.bib">[ref]</a> </td><td valign="top"> Fast-RCNN </td><td valign="top"> Selective search for LiDAR and RGB image separately. </td><td valign="top"> At RP </td><td valign="top"> Ensemble: joint RP are fed to RGB image based CNN. </td><td valign="top"> Late </td><td valign="top"> KITTI </td></tr>

<tr><td valign="top"> Mees <i>et al.</i>, 2016 
    <a href="https://ieeexplore.ieee.org/abstract/document/7759048/">[pdf]</a><a href="/ref/mees2016choosing.bib">[ref]</a>
    </td><td valign="top"> RGB-D camera </td><td valign="top"> 2D Pedestrian </td><td valign="top"> RGB image, depth image from depth camera, optical flow. Each processed by GoogLeNet </td><td valign="top"> Fast-RCNN </td><td valign="top"> Dense multi-scale sliding window for RGB image </td><td valign="top"> After RP </td><td valign="top"> Mixture of Experts </td><td valign="top"> Late </td><td valign="top"> RGB-D People Unihall Dataset, InOutDoor RGB-D People Dataset. </td></tr> 

<tr><td valign="top"> Wagner <i>et al.</i>, 2016 
    <a href="https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2016-118.pdf">[pdf]</a><a href="/ref/wagner2016multispectral.bib">[ref]</a>
    </td><td valign="top"> Vision camera, thermal camera </td><td valign="top"> 2D Pedestrian </td><td valign="top"> RGB image, thermal image. Each processed by CaffeeNet </td><td valign="top"> R-CNN </td><td valign="top"> ACF+T+THOG detector </td><td valign="top"> After RP </td><td valign="top"> Feature concatenation </td><td valign="top"> Early, Late </td><td valign="top"> KAIST Pedestrian Dataset </td></tr>

<tr><td valign="top"> Liu <i>et al.</i>, 2016 
    <a href="https://dx.doi.org/10.5244/C.30.73">[pdf]</a><a href="/ref/liu2016bmvc.bib">[ref]</a>
    </td><td valign="top"> Vision camera, thermal camera </td><td valign="top"> 2D Pedestrian </td><td valign="top"> RGB image, thermal image. Each processed by NiN network </td><td valign="top"> Faster-RCNN </td><td valign="top"> RPN with fused (or separate) features </td><td valign="top"> Before and after RP </td><td valign="top"> Feature concatenation, average mean, Score fusion (Cascaded CNN) </td><td valign="top"> Early, Middle, Late </td><td valign="top"> KAIST Pedestrian Dataset </td></tr>
</table>


      <footer class="site-footer">
        
        <span class="site-footer-credits">
		(c) Robert Bosch GmbH 2019. All rights reserved. <a href="http://www.bosch.com/research">www.bosch.com/research</a>.
		<p></p>
        This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>